{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47Hiqh3GQe0q"
      },
      "outputs": [],
      "source": [
        "#1. Import and Install Dependencies\n",
        "# Updated versions for 2025\n",
        "!pip install tensorflow>=2.17.0 opencv-python mediapipe>=0.10.0 scikit-learn matplotlib numpy\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import mediapipe as mp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Keypoints using MP Holistic\n",
        "mp_holistic = mp.solutions.holistic  # Holistic model\n",
        "mp_drawing = mp.solutions.drawing_utils  # Drawing utilities\n",
        "\n",
        "def mediapipe_detection(image, model):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # COLOR CONVERSION BGR 2 RGB\n",
        "    image.flags.writeable = False                   # Image is no longer writeable\n",
        "    results = model.process(image)                  # Make prediction\n",
        "    image.flags.writeable = True                    # Image is now writeable\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # COLOR CONVERSION RGB 2 BGR\n",
        "    return image, results\n",
        "\n",
        "def draw_landmarks(image, results):\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS)  # Draw face connections\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)   # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)  # Draw left hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
        "\n",
        "def draw_styled_landmarks(image, results):\n",
        "    # Draw face connections\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
        "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
        "                             )\n",
        "    # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "    # Draw left hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "    # Draw right hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "# Set mediapipe model\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "    while cap.isOpened():\n",
        "        # Read feed\n",
        "        ret, frame = cap.read()\n",
        "        # Make detections\n",
        "        image, results = mediapipe_detection(frame, holistic)\n",
        "        print(results)\n",
        "\n",
        "        # Draw landmarks\n",
        "        draw_styled_landmarks(image, results)\n",
        "        # Show to screen\n",
        "        cv2.imshow('OpenCV Feed', image)\n",
        "        # Break gracefully\n",
        "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "            break\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "draw_landmarks(frame, results)\n",
        "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n"
      ],
      "metadata": {
        "id": "SS8edb-OQiQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Extract Keypoint Values\n",
        "# Check available landmarks\n",
        "if results.left_hand_landmarks:\n",
        "    print(len(results.left_hand_landmarks.landmark))  # Should be 21\n",
        "\n",
        "# Extract keypoints with improved error handling\n",
        "def extract_keypoints(results):\n",
        "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
        "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "    return np.concatenate([pose, face, lh, rh])\n",
        "\n",
        "result_test = extract_keypoints(results)\n",
        "print(result_test.shape)\n",
        "\n",
        "# Save and load test\n",
        "np.save('0', result_test)\n",
        "loaded_result = np.load('0.npy')\n",
        "print(\"Successfully saved and loaded keypoints\")\n"
      ],
      "metadata": {
        "id": "c_nRWiNDQopq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Setup Folders for Collection\n",
        "# Path for exported data, numpy arrays\n",
        "DATA_PATH = os.path.join('MP_Data')\n",
        "\n",
        "# Actions that we try to detect\n",
        "actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
        "\n",
        "# Thirty videos worth of data\n",
        "no_sequences = 30\n",
        "\n",
        "# Videos are going to be 30 frames in length\n",
        "sequence_length = 30\n",
        "\n",
        "# Folder start\n",
        "start_folder = 30\n",
        "\n",
        "# Create directories with improved error handling\n",
        "for action in actions:\n",
        "    action_path = os.path.join(DATA_PATH, action)\n",
        "    if os.path.exists(action_path):\n",
        "        dirmax = np.max(np.array(os.listdir(action_path)).astype(int)) if os.listdir(action_path) else 0\n",
        "    else:\n",
        "        dirmax = 0\n",
        "        os.makedirs(action_path, exist_ok=True)\n",
        "\n",
        "    for sequence in range(1, no_sequences+1):\n",
        "        try:\n",
        "            os.makedirs(os.path.join(DATA_PATH, action, str(dirmax+sequence)), exist_ok=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating directory: {e}\")\n"
      ],
      "metadata": {
        "id": "HMTazkYuQwKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Collect Keypoint Values for Training and Testing\n",
        "cap = cv2.VideoCapture(0)\n",
        "# Set mediapipe model\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "\n",
        "    # Loop through actions\n",
        "    for action in actions:\n",
        "        # Loop through sequences aka videos\n",
        "        for sequence in range(start_folder, start_folder+no_sequences):\n",
        "            # Loop through video length aka sequence length\n",
        "            for frame_num in range(sequence_length):\n",
        "                # Read feed\n",
        "                ret, frame = cap.read()\n",
        "\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Make detections\n",
        "                image, results = mediapipe_detection(frame, holistic)\n",
        "                # Draw landmarks\n",
        "                draw_styled_landmarks(image, results)\n",
        "\n",
        "                # Apply wait logic\n",
        "                if frame_num == 0:\n",
        "                    cv2.putText(image, 'STARTING COLLECTION', (120,200),\n",
        "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
        "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
        "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "                    # Show to screen\n",
        "                    cv2.imshow('OpenCV Feed', image)\n",
        "                    cv2.waitKey(500)\n",
        "                else:\n",
        "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
        "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "                    # Show to screen\n",
        "                    cv2.imshow('OpenCV Feed', image)\n",
        "\n",
        "                # Export keypoints\n",
        "                keypoints = extract_keypoints(results)\n",
        "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
        "                np.save(npy_path, keypoints)\n",
        "\n",
        "                # Break gracefully\n",
        "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "                    break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "uv4RayqnQ0sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Preprocess Data and Create Labels and Features\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create label map\n",
        "label_map = {label:num for num, label in enumerate(actions)}\n",
        "print(label_map)\n",
        "\n",
        "sequences, labels = [], []\n",
        "for action in actions:\n",
        "    action_path = os.path.join(DATA_PATH, action)\n",
        "    for sequence in np.array(os.listdir(action_path)).astype(int):\n",
        "        window = []\n",
        "        for frame_num in range(sequence_length):\n",
        "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
        "            window.append(res)\n",
        "        sequences.append(window)\n",
        "        labels.append(label_map[action])\n",
        "\n",
        "print(f\"Sequences shape: {np.array(sequences).shape}\")\n",
        "print(f\"Labels shape: {np.array(labels).shape}\")\n",
        "\n",
        "X = np.array(sequences)\n",
        "y = tf.keras.utils.to_categorical(labels).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
        "print(f\"Test set shape: {y_test.shape}\")\n"
      ],
      "metadata": {
        "id": "lXyWjVFfQ7HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Build and Train LSTM Neural Network\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Create log directory\n",
        "log_dir = os.path.join('Logs')\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Callbacks for better training\n",
        "tb_callback = TensorBoard(log_dir=log_dir)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20, min_lr=0.001)\n",
        "\n",
        "# Build improved model\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=True, activation='relu', input_shape=(30, 1662)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(128, return_sequences=True, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64, return_sequences=False, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(actions.shape[0], activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile with updated optimizer\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['categorical_accuracy']\n",
        ")\n",
        "\n",
        "# Train with validation split and callbacks\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=500,  # Reduced from 2000\n",
        "    validation_split=0.2,\n",
        "    callbacks=[tb_callback, early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "Z0-4oWWmQ_At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Make Predictions\n",
        "# Make predictions\n",
        "res = model.predict(X_test, verbose=0)\n",
        "print(\"Prediction:\", actions[np.argmax(res[0])])\n",
        "print(\"Actual:\", actions[np.argmax(y_test[0])])\n"
      ],
      "metadata": {
        "id": "ub0LaR4qRCoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Evaluation using Confusion Matrix and Accuracy\n",
        "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "# Make predictions\n",
        "yhat = model.predict(X_test, verbose=0)\n",
        "ytrue = np.argmax(y_test, axis=1).tolist()\n",
        "yhat_classes = np.argmax(yhat, axis=1).tolist()\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(multilabel_confusion_matrix(ytrue, yhat_classes))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(ytrue, yhat_classes)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(ytrue, yhat_classes, target_names=actions))\n"
      ],
      "metadata": {
        "id": "pOuS9gqNRMnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Test in Real Time\n",
        "from scipy import stats\n",
        "\n",
        "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
        "\n",
        "def prob_viz(res, actions, input_frame, colors):\n",
        "    output_frame = input_frame.copy()\n",
        "    for num, prob in enumerate(res):\n",
        "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
        "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
        "    return output_frame\n",
        "\n",
        "# Real-time detection\n",
        "sequence = []\n",
        "sentence = []\n",
        "predictions = []\n",
        "threshold = 0.5\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Set mediapipe model\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "    while cap.isOpened():\n",
        "        # Read feed\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Make detections\n",
        "        image, results = mediapipe_detection(frame, holistic)\n",
        "\n",
        "        # Draw landmarks\n",
        "        draw_styled_landmarks(image, results)\n",
        "\n",
        "        # Prediction logic\n",
        "        keypoints = extract_keypoints(results)\n",
        "        sequence.append(keypoints)\n",
        "        sequence = sequence[-30:]  # Keep only last 30 frames\n",
        "\n",
        "        if len(sequence) == 30:\n",
        "            res = model.predict(np.expand_dims(sequence, axis=0), verbose=0)[0]\n",
        "            predictions.append(np.argmax(res))\n",
        "\n",
        "            # Visualization logic\n",
        "            if len(predictions) >= 10:\n",
        "                if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
        "                    if res[np.argmax(res)] > threshold:\n",
        "                        if len(sentence) > 0:\n",
        "                            if actions[np.argmax(res)] != sentence[-1]:\n",
        "                                sentence.append(actions[np.argmax(res)])\n",
        "                        else:\n",
        "                            sentence.append(actions[np.argmax(res)])\n",
        "\n",
        "            if len(sentence) > 5:\n",
        "                sentence = sentence[-5:]\n",
        "\n",
        "            # Visualize probabilities\n",
        "            image = prob_viz(res, actions, image, colors)\n",
        "\n",
        "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
        "        cv2.putText(image, ' '.join(sentence), (3,30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "        # Show to screen\n",
        "        cv2.imshow('OpenCV Feed', image)\n",
        "\n",
        "        # Break gracefully\n",
        "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "WwVIcj2DRSjV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}